# Prompt Robustness Test for LLMs
### KO
이 레포지토리에는 동일한 의미를 가졌지만 구조/표현만 조금 달라진 프롬프트가 주어졌을 때 대규모 언어 모델(LLM)의 결과가 얼마나 균일한지 테스트하기 위한 Python 코드와 결과가 포함되어 있습니다. 프롬프트 취약성, 즉 프롬프트 구조의 작은 변화에 대한 LLM의 민감성 문제를 부각시키는 것이 목적입니다.
이 프로젝트는 "State of What Art? A Call for Multi-Prompt LLM Evaluation"(https://arxiv.org/pdf/2401.00595.pdf) 논문에서 영감을 받았습니다. 이 논문은 프롬프트 구조의 사소한 변화에 대한 LLM의 민감성을 강조합니다.

### EN
This repository contains Python code and results for testing the robustness of Large Language Models (LLMs) when given paraphrased prompts. It aims to highlight the issue of prompt brittleness, which is the sensitivity of LLMs to minor changes in prompt structure.
Inspired by the paper "State of What Art? A Call for Multi-Prompt LLM Evaluation" (https://arxiv.org/pdf/2401.00595.pdf), which highlights the sensitivity of LLMs to minor changes in prompt structure.
