# Prompt Robustness Test for LLMs
This repository contains Python code and results for testing the robustness of Large Language Models (LLMs) when given paraphrased prompts. It aims to highlight the issue of prompt brittleness, which is the sensitivity of LLMs to minor changes in prompt structure.
Inspired by the paper "State of What Art? A Call for Multi-Prompt LLM Evaluation" (https://arxiv.org/pdf/2401.00595.pdf), which highlights the sensitivity of LLMs to minor changes in prompt structure.
